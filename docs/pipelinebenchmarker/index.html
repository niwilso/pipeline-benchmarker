<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pipelinebenchmarker API documentation</title>
<meta name="description" content="This library allows you to easily benchmark pipeline performance metrics â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>pipelinebenchmarker</code></h1>
</header>
<section id="section-intro">
<p>This library allows you to easily benchmark pipeline performance metrics.</p>
<h1 id="using-the-pipelinebenchmarker-class">Using the PipelineBenchmarker class</h1>
<p>The <code>PipelineBenchmarker</code> class assists in automatically tracking benchmarking values of interest in any <a href="https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines">Azure Machine Learning (AML) pipeline</a>. This includes compute metadata, step duration time, maximum number of nodes used, and any custom parameter values designated by the user.</p>
<p>This document provides an overview of the purpose of the class and examples of how to implement it. Fully functional example code is available at <code>mlops/example_pipeline/*</code> and code-specific class documentation at <a href="./benchmark_utils.html"><code>benchmark_utils.html</code></a>.</p>
<h2 id="benefits-of-benchmarking-your-pipeline">Benefits of benchmarking your pipeline</h2>
<p>When developing an MLOps solution, the team may need to <strong>experiment with various compute configurations and miscellaneous parameters</strong> before settling on what works best for the particular use case.</p>
<p>The <code>PipelineBenchmarker</code> tool makes performing these comparisons easier by automatically tracking benchmark values of interest separate from traditional model metrics.</p>
<blockquote>
<p><em>While model performance metrics may certainly be logged with the <code>PipelineBenchmarker</code> class, that is not the intended purpose.</em></p>
</blockquote>
<p>By tracking values such as compute configuration (e.g., compute target name, size, priority, region), number of nodes used, and the number of time to complete each step and the total pipeline, we can gain a clear understanding of (1) what steps are the most computationally heavy and (2) what parameters and compute configurations we may want to change to optimize for either speed or cost.</p>
<p>The class can be particularly helpful in cases where calls to Azure Cognitive Services need to be logged for cost estimation. Additional values such as "number of calls to the Custom Vision service" or "duration of audio sent to the Custom Speech service" may easily be passed into the <code>PipelineBenchmarker</code> object.</p>
<h2 id="how-to-use-the-pipelinebenchmarker-class">How to use the PipelineBenchmarker class</h2>
<p>The general structure of how to use the <code>PipelineBenchmarker</code> class is as follows:</p>
<ol>
<li>For each of pipeline step's Python script:<ul>
<li>Import the <code>PipelineBenchmarker</code> class</li>
<li>Instantiate a <code>PipelineBenchmarker</code> object at the start of your step</li>
<li>Save benchmark values at the end of your step by calling the <code>.save_step_benchmark(step_name="", experiment_output_dir="", benchmark_dict={})</code> method on your object with the appropriate argument values. Step duration and number of nodes used are automatically saved through this step, and additional values may be stored as a dictionary through the optional <code>benchmark_dict</code> argument.</li>
<li>For full example code demonstrating this implementation, see <code>prepare_data.py</code> and <code>process_data.py</code> in <code>mlops/example_pipeline/steps/</code></li>
</ul>
</li>
<li>Create an additional AML pipeline step to perform the pipeline benchmarking consolidation (see <code>mlops/example_pipeline/steps/generate_benchmark_report.py</code>)</li>
<li>Add this benchmark report generation step to your pipeline creation script, and ensure it is the last step to execute (see <code>mlops/example_pipeline/Example_Pipeline.ipynb</code>)</li>
</ol>
<h2 id="autogenerated-benchmark-reports-are-saved-to-blob">Autogenerated benchmark reports are saved to blob</h2>
<p>With the addition of the final benchmark report generation pipeline step, consolidated benchmarks are written out.</p>
<p>The two outputs are as follows:</p>
<ol>
<li>A singular <code>benchmark_&lt;pipeline run ID&gt;.json</code> is generated and saved per pipeline run. This is stored in <code>pipeline_benchmarks/individual_runs/</code> in the mounted Azure blob storage.</li>
<li>Additional rows are added onto <code>pipeline_benchmarks/benchmarks.csv</code> in the mounted Azure blob storage, with each row containing values for each step benchmarked in the pipeline run (excluding the <code>generate_benchmark_report.py</code> step).</li>
</ol>
<p>Example output are available in <code>docs/example_output/*</code>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This library allows you to easily benchmark pipeline performance metrics.

# Using the PipelineBenchmarker class

The `PipelineBenchmarker` class assists in automatically tracking benchmarking values of interest in any [Azure Machine Learning (AML) pipeline](https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines). This includes compute metadata, step duration time, maximum number of nodes used, and any custom parameter values designated by the user.

This document provides an overview of the purpose of the class and examples of how to implement it. Fully functional example code is available at `mlops/example_pipeline/*` and code-specific class documentation at [`benchmark_utils.html`](./benchmark_utils.html).

## Benefits of benchmarking your pipeline

When developing an MLOps solution, the team may need to **experiment with various compute configurations and miscellaneous parameters** before settling on what works best for the particular use case.

The `PipelineBenchmarker` tool makes performing these comparisons easier by automatically tracking benchmark values of interest separate from traditional model metrics.

&gt; *While model performance metrics may certainly be logged with the `PipelineBenchmarker` class, that is not the intended purpose.*

By tracking values such as compute configuration (e.g., compute target name, size, priority, region), number of nodes used, and the number of time to complete each step and the total pipeline, we can gain a clear understanding of (1) what steps are the most computationally heavy and (2) what parameters and compute configurations we may want to change to optimize for either speed or cost.

The class can be particularly helpful in cases where calls to Azure Cognitive Services need to be logged for cost estimation. Additional values such as &#34;number of calls to the Custom Vision service&#34; or &#34;duration of audio sent to the Custom Speech service&#34; may easily be passed into the `PipelineBenchmarker` object.

## How to use the PipelineBenchmarker class

The general structure of how to use the `PipelineBenchmarker` class is as follows:

1. For each of pipeline step&#39;s Python script:
    - Import the `PipelineBenchmarker` class
    - Instantiate a `PipelineBenchmarker` object at the start of your step
    - Save benchmark values at the end of your step by calling the `.save_step_benchmark(step_name=&#34;&#34;, experiment_output_dir=&#34;&#34;, benchmark_dict={})` method on your object with the appropriate argument values. Step duration and number of nodes used are automatically saved through this step, and additional values may be stored as a dictionary through the optional `benchmark_dict` argument.
    - For full example code demonstrating this implementation, see `prepare_data.py` and `process_data.py` in `mlops/example_pipeline/steps/`
2. Create an additional AML pipeline step to perform the pipeline benchmarking consolidation (see `mlops/example_pipeline/steps/generate_benchmark_report.py`)
3. Add this benchmark report generation step to your pipeline creation script, and ensure it is the last step to execute (see `mlops/example_pipeline/Example_Pipeline.ipynb`)

## Autogenerated benchmark reports are saved to blob

With the addition of the final benchmark report generation pipeline step, consolidated benchmarks are written out.

The two outputs are as follows:

1. A singular `benchmark_&lt;pipeline run ID&gt;.json` is generated and saved per pipeline run. This is stored in `pipeline_benchmarks/individual_runs/` in the mounted Azure blob storage.
2. Additional rows are added onto `pipeline_benchmarks/benchmarks.csv` in the mounted Azure blob storage, with each row containing values for each step benchmarked in the pipeline run (excluding the `generate_benchmark_report.py` step).

Example output are available in `docs/example_output/*`.

&#34;&#34;&#34;</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="pipelinebenchmarker.benchmark_utils" href="benchmark_utils.html">pipelinebenchmarker.benchmark_utils</a></code></dt>
<dd>
<div class="desc"><p>A modular class designed for
benchmarking pipeline steps and overall run.</p></div>
</dd>
<dt><code class="name"><a title="pipelinebenchmarker.timestamp" href="timestamp.html">pipelinebenchmarker.timestamp</a></code></dt>
<dd>
<div class="desc"><p>General timestamp preprocessing utilities</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#using-the-pipelinebenchmarker-class">Using the PipelineBenchmarker class</a><ul>
<li><a href="#benefits-of-benchmarking-your-pipeline">Benefits of benchmarking your pipeline</a></li>
<li><a href="#how-to-use-the-pipelinebenchmarker-class">How to use the PipelineBenchmarker class</a></li>
<li><a href="#autogenerated-benchmark-reports-are-saved-to-blob">Autogenerated benchmark reports are saved to blob</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="pipelinebenchmarker.benchmark_utils" href="benchmark_utils.html">pipelinebenchmarker.benchmark_utils</a></code></li>
<li><code><a title="pipelinebenchmarker.timestamp" href="timestamp.html">pipelinebenchmarker.timestamp</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>