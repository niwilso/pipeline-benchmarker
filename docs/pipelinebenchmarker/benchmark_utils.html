<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pipelinebenchmarker.benchmark_utils API documentation</title>
<meta name="description" content="A modular class designed for
benchmarking pipeline steps and overall run." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pipelinebenchmarker.benchmark_utils</code></h1>
</header>
<section id="section-intro">
<p>A modular class designed for
benchmarking pipeline steps and overall run.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
A modular class designed for
benchmarking pipeline steps and overall run.
&#34;&#34;&#34;
import os
from os.path import join
from pathlib import Path
import tempfile
from typing import Tuple
import datetime
import warnings
import json
import pandas as pd
from azureml.core import Run
from azureml.core.compute import ComputeTarget
from pipelinebenchmarker.timestamp import format_seconds_to_timestamp


class PipelineBenchmarker:
    &#34;&#34;&#34;PipelineBenchmarker class&#34;&#34;&#34;

    def __init__(self, mounted_blob_dir: str = &#34;&#34;) -&gt; None:

        &#34;&#34;&#34;Constructs an instance of the PipelineBenchmarker class.

        Parameters
        ----------
        mounted_blob_dir : str
            (Optional) Path to mounted Azure blob storage
        &#34;&#34;&#34;
        self.mounted_blob_dir = mounted_blob_dir
        if len(mounted_blob_dir) &gt; 0:
            self.blob_storage_mounted = True
        else:
            self.blob_storage_mounted = False

        self.step_duration_start = datetime.datetime.utcnow()
        self.run = Run.get_context()
        self.parent_run = Run.get_context().parent
        self.step_details = self.run.get_details()
        self.pipeline_details = self.parent_run.get_details()
        self.workspace = self.run.experiment.workspace
        self.pipeline_benchmarks = {}

    def __get_n_nodes(self) -&gt; Tuple[int, str]:
        &#34;&#34;&#34;Gets the number of active nodes in AML compute.

        Returns
        -------
        n_nodes : int
            Number of currently active nodes
        compute_name : str
            Name of the compute cluster
        &#34;&#34;&#34;
        # Get workspace info
        compute_name = self.step_details[&#34;target&#34;]

        # Retrieve relevant compute from current workspace
        aml_compute = ComputeTarget(self.workspace, compute_name)
        n_nodes = int(aml_compute.get_status().current_node_count)

        return n_nodes, compute_name

    def __get_compute_config(self) -&gt; Tuple[str, str, str, str]:
        &#34;&#34;&#34;Retrieves the compute target&#39;s configuration.

        Returns
        -------
        compute_name : str
            Name of the compute cluster
        vm_size : str
            Size of the compute cluster (e.g., &#34;STANDARD_DS_v2&#34;)
        vm_priority : str
            Priority of the compute target (dedicated or low priority)
        region : str
            Location of the compute cluster
        &#34;&#34;&#34;
        # Get workspace info
        compute_name = self.step_details[&#34;target&#34;]
        aml_compute = ComputeTarget(self.workspace, compute_name)

        # Retrieve properties
        vm_size = aml_compute.vm_size
        vm_priority = aml_compute.vm_priority
        region = aml_compute.cluster_location

        return compute_name, vm_size, vm_priority, region

    def __get_step_benchmark(
        self, experiment_output_dir: str, filename: str
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Gets the benchmark data from a step.

        Parameters
        ----------
        experiment_output_dir : str
            Name of output folder for the experiment
        filename : str
            Name of step benchmark file

        Returns
        -------
        df_metadata : pd.DataFrame
            Dataframe containing benchmark values from given step
        &#34;&#34;&#34;
        with tempfile.NamedTemporaryFile(suffix=&#34;.csv&#34;) as filepath_temp:
            filepath_steps_metadata = join(experiment_output_dir, filename)
            run = Run.get_context()
            run.parent.download_file(
                filepath_steps_metadata,
                filepath_temp.name,
            )
            df_metadata = pd.read_csv(filepath_temp.name)
            return df_metadata

    def get_file_size_and_count(
        self, directory: str, file_extension: str
    ) -&gt; Tuple[float, int]:
        &#34;&#34;&#34;Gets the total size of all files in a Azure blob storage directory and the number of files.

        Parameters
        ----------
        directory : str
            Path to directory containing files (excluding path to mounted storage base directory)
        file_extension : str
            Only files with the specified file extension will be counted (e.g., &#34;.mp4&#34;)

        Returns
        -------
        total_size : float
            Total size of all counted files, in kilobytes
        file_count : int
            Total number of files matching the specified file extension
        &#34;&#34;&#34;
        if self.blob_storage_mounted:
            directory = join(self.mounted_blob_dir, directory)
            total_size = 0
            file_count = 0
            for filename in os.listdir(directory):
                if file_extension in filename[-len(file_extension) :]:
                    file_size = os.path.getsize(join(directory, filename)) / 1000
                    total_size += file_size
                    file_count += 1
        else:
            warnings.warn(&#34;PipelineBenchmarker does not have access to blob storage&#34;)
            total_size = 0
            file_count = 0
        return total_size, file_count

    def save_step_benchmark(
        self,
        step_name: str,
        experiment_output_dir: str,
        benchmark_dict: dict = {},
    ) -&gt; None:
        &#34;&#34;&#34;Save current step&#39;s benchmarking data.

        Parameters
        ----------
        step_name : str
            Name of the pipeline step (e.g., &#34;extract_audio&#34;)
        experiment_output_dir : str
            Directory at pipeline experiment level to store output in AML (e.g., &#34;output&#34;)
        benchmark_dict : dict
            (Optional) Dictionary containing additional metrics to save (should be flat, not nested).
            Duration, node count, and compute cluster name are automatically included so there is no need to include them in the input.
            Dictionary values should be contained within brackets (e.g., {&#34;n_files&#34;: [34]}).
        &#34;&#34;&#34;
        os.makedirs(experiment_output_dir, exist_ok=True)

        # Check if benchmark_dict is nested
        is_nested = any(isinstance(i, dict) for i in benchmark_dict.values())
        if is_nested:
            warnings.warn(
                &#34;Step benchmark values may not save correctly in the final consolidation step if benchmark_dict is nested. Please flatten the dictionary.&#34;
            )

        # Add duration and node count to dictionary
        duration_in_sec = (
            datetime.datetime.utcnow() - self.step_duration_start
        ).total_seconds()
        benchmark_dict[&#34;duration&#34;] = [format_seconds_to_timestamp(duration_in_sec)]
        n_nodes, compute_name = self.__get_n_nodes()
        benchmark_dict[&#34;n_nodes&#34;] = [n_nodes]
        benchmark_dict[&#34;compute_name&#34;] = [compute_name]
        benchmark_dict[&#34;pipeline_step&#34;] = [step_name]

        # Convert dictionary to dataframe
        df_metadata = pd.DataFrame(benchmark_dict)
        filename = f&#34;benchmark_{step_name}.csv&#34;
        metadata_filepath = join(experiment_output_dir, filename)
        df_metadata.to_csv(metadata_filepath, header=True)

        # Write to pipeline run output such that it is accessible in the final benchmarking step
        self.run.parent.upload_folder(experiment_output_dir, experiment_output_dir)
        print(
            f&#34;{filename} written to AML pipeline experiment output directory: {experiment_output_dir}&#34;
        )

    def save_pipeline_benchmark(
        self, pipeline_steps: list, experiment_output_dir: str
    ) -&gt; None:
        &#34;&#34;&#34;Consolidates all step benchmarks into one overall pipeline benchmark json.
        Uploads final json to AML output and to Azure blob storage if mounted.

        Parameters
        ----------
        pipeline_steps : list
            List of step names as defined in the pipeline (e.g., [&#34;extract_audio&#34;, &#34;process_audio&#34;])
        experiment_output_dir : str
            Directory at pipeline experiment level to store output in AML (e.g., &#34;output&#34;)
        &#34;&#34;&#34;
        os.makedirs(experiment_output_dir, exist_ok=True)

        # Initialize
        run_id = self.run.parent.id
        t_pipeline_start = datetime.datetime.strptime(
            self.pipeline_details[&#34;startTimeUtc&#34;], &#34;%Y-%m-%dT%H:%M:%S.%fZ&#34;
        )
        pipeline_duration = (
            datetime.datetime.utcnow() - t_pipeline_start
        ).total_seconds()
        compute_name, vm_size, vm_priority, region = self.__get_compute_config()

        pipeline_benchmarks = {
            &#34;datetime&#34;: datetime.datetime.now().strftime(&#34;%Y-%m-%d %H:%M:%S&#34;),
            &#34;run_id&#34;: run_id,
            &#34;pipeline_duration&#34;: format_seconds_to_timestamp(pipeline_duration),
            &#34;compute_config&#34;: {
                &#34;name&#34;: compute_name,
                &#34;vm_size&#34;: vm_size,
                &#34;vm_priority&#34;: vm_priority,
                &#34;region&#34;: region,
            },
            &#34;steps&#34;: {},
        }
        self.pipeline_benchmarks = pipeline_benchmarks

        # Add in benchmarks from each step
        max_nodes_used = 0
        for step in pipeline_steps:
            df_step_benchmark = self.__get_step_benchmark(
                experiment_output_dir,
                f&#34;benchmark_{step}.csv&#34;,
            )
            # Format into dictionary
            step_dictionary = df_step_benchmark.to_dict(orient=&#34;records&#34;)[0]
            step_dictionary.pop(&#34;Unnamed: 0&#34;)  # remove index
            pipeline_benchmarks[&#34;steps&#34;][step] = step_dictionary

            if step_dictionary[&#34;n_nodes&#34;] &gt; max_nodes_used:
                max_nodes_used = step_dictionary[&#34;n_nodes&#34;]

        pipeline_benchmarks[&#34;max_nodes_used&#34;] = max_nodes_used

        # Write to AML output
        with open(join(experiment_output_dir, &#34;benchmarks.json&#34;), &#34;w&#34;) as json_file:
            json.dump(pipeline_benchmarks, json_file)
        self.run.parent.upload_folder(experiment_output_dir, experiment_output_dir)
        print(
            f&#34;benchmarks.json written to AML pipeline experiment output directory: {experiment_output_dir}&#34;
        )

        # Write to mounted azure blob storage
        if self.blob_storage_mounted:
            folder_path = join(
                self.mounted_blob_dir, &#34;pipeline_benchmarks&#34;, &#34;individual_runs&#34;
            )
            os.makedirs(folder_path, exist_ok=True)
            filename = f&#34;benchmark_{run_id}.json&#34;

            with open(join(folder_path, filename), &#34;w&#34;) as output_file:
                json.dump(pipeline_benchmarks, output_file)
            print(f&#34;{filename} written to {folder_path}&#34;)

    def update_benchmark_table(self, table_filepath: str) -&gt; None:
        &#34;&#34;&#34;Writes or updates a table storing all the benchmarking info.

        Parameters
        ----------
        table_filepath : str
            Path to the table which contains or will contain benchmarks for all runs. It is recommended this is a filepath to somewhere in mounted blob storage.
        &#34;&#34;&#34;
        new_benchmark = self.pipeline_benchmarks

        pipeline_benchmarks = {
            &#34;datetime&#34;: [new_benchmark[&#34;datetime&#34;]],
            &#34;run_id&#34;: [new_benchmark[&#34;run_id&#34;]],
            &#34;pipeline_duration&#34;: [new_benchmark[&#34;pipeline_duration&#34;]],
            &#34;compute_name&#34;: [new_benchmark[&#34;compute_config&#34;][&#34;name&#34;]],
            &#34;compute_size&#34;: [new_benchmark[&#34;compute_config&#34;][&#34;vm_size&#34;]],
            &#34;compute_priority&#34;: [new_benchmark[&#34;compute_config&#34;][&#34;vm_priority&#34;]],
            &#34;compute_region&#34;: [new_benchmark[&#34;compute_config&#34;][&#34;region&#34;]],
        }

        df_append = pd.DataFrame()
        for step in new_benchmark[&#34;steps&#34;].keys():
            step_benchmarks = pipeline_benchmarks.copy()
            step_benchmarks[&#34;step_name&#34;] = [step]
            for step_val in new_benchmark[&#34;steps&#34;][step].keys():
                if step_val not in [
                    &#34;compute_name&#34;,
                    &#34;compute_size&#34;,
                    &#34;compute_priority&#34;,
                    &#34;compute_region&#34;,
                    &#34;pipeline_step&#34;,
                ]:
                    if step_val in [&#34;duration&#34;, &#34;n_nodes&#34;]:
                        step_benchmarks[step_val] = new_benchmark[&#34;steps&#34;][step][
                            step_val
                        ]
                    else:
                        # Preface specific (non-default) step metrics with step name
                        step_benchmarks[f&#34;{step}_{step_val}&#34;] = new_benchmark[&#34;steps&#34;][
                            step
                        ][step_val]

            df_temp = pd.DataFrame(step_benchmarks)
            df_append = df_append.append(df_temp, ignore_index=True)

        # Check if file exists
        if Path(table_filepath).is_file():
            # file exists
            df_append.to_csv(table_filepath, mode=&#34;a&#34;, header=False, index=False)
        else:
            df_append.to_csv(table_filepath, header=True, index=False)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pipelinebenchmarker.benchmark_utils.PipelineBenchmarker"><code class="flex name class">
<span>class <span class="ident">PipelineBenchmarker</span></span>
<span>(</span><span>mounted_blob_dir: str = '')</span>
</code></dt>
<dd>
<div class="desc"><p>PipelineBenchmarker class</p>
<p>Constructs an instance of the PipelineBenchmarker class.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mounted_blob_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>(Optional) Path to mounted Azure blob storage</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PipelineBenchmarker:
    &#34;&#34;&#34;PipelineBenchmarker class&#34;&#34;&#34;

    def __init__(self, mounted_blob_dir: str = &#34;&#34;) -&gt; None:

        &#34;&#34;&#34;Constructs an instance of the PipelineBenchmarker class.

        Parameters
        ----------
        mounted_blob_dir : str
            (Optional) Path to mounted Azure blob storage
        &#34;&#34;&#34;
        self.mounted_blob_dir = mounted_blob_dir
        if len(mounted_blob_dir) &gt; 0:
            self.blob_storage_mounted = True
        else:
            self.blob_storage_mounted = False

        self.step_duration_start = datetime.datetime.utcnow()
        self.run = Run.get_context()
        self.parent_run = Run.get_context().parent
        self.step_details = self.run.get_details()
        self.pipeline_details = self.parent_run.get_details()
        self.workspace = self.run.experiment.workspace
        self.pipeline_benchmarks = {}

    def __get_n_nodes(self) -&gt; Tuple[int, str]:
        &#34;&#34;&#34;Gets the number of active nodes in AML compute.

        Returns
        -------
        n_nodes : int
            Number of currently active nodes
        compute_name : str
            Name of the compute cluster
        &#34;&#34;&#34;
        # Get workspace info
        compute_name = self.step_details[&#34;target&#34;]

        # Retrieve relevant compute from current workspace
        aml_compute = ComputeTarget(self.workspace, compute_name)
        n_nodes = int(aml_compute.get_status().current_node_count)

        return n_nodes, compute_name

    def __get_compute_config(self) -&gt; Tuple[str, str, str, str]:
        &#34;&#34;&#34;Retrieves the compute target&#39;s configuration.

        Returns
        -------
        compute_name : str
            Name of the compute cluster
        vm_size : str
            Size of the compute cluster (e.g., &#34;STANDARD_DS_v2&#34;)
        vm_priority : str
            Priority of the compute target (dedicated or low priority)
        region : str
            Location of the compute cluster
        &#34;&#34;&#34;
        # Get workspace info
        compute_name = self.step_details[&#34;target&#34;]
        aml_compute = ComputeTarget(self.workspace, compute_name)

        # Retrieve properties
        vm_size = aml_compute.vm_size
        vm_priority = aml_compute.vm_priority
        region = aml_compute.cluster_location

        return compute_name, vm_size, vm_priority, region

    def __get_step_benchmark(
        self, experiment_output_dir: str, filename: str
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Gets the benchmark data from a step.

        Parameters
        ----------
        experiment_output_dir : str
            Name of output folder for the experiment
        filename : str
            Name of step benchmark file

        Returns
        -------
        df_metadata : pd.DataFrame
            Dataframe containing benchmark values from given step
        &#34;&#34;&#34;
        with tempfile.NamedTemporaryFile(suffix=&#34;.csv&#34;) as filepath_temp:
            filepath_steps_metadata = join(experiment_output_dir, filename)
            run = Run.get_context()
            run.parent.download_file(
                filepath_steps_metadata,
                filepath_temp.name,
            )
            df_metadata = pd.read_csv(filepath_temp.name)
            return df_metadata

    def get_file_size_and_count(
        self, directory: str, file_extension: str
    ) -&gt; Tuple[float, int]:
        &#34;&#34;&#34;Gets the total size of all files in a Azure blob storage directory and the number of files.

        Parameters
        ----------
        directory : str
            Path to directory containing files (excluding path to mounted storage base directory)
        file_extension : str
            Only files with the specified file extension will be counted (e.g., &#34;.mp4&#34;)

        Returns
        -------
        total_size : float
            Total size of all counted files, in kilobytes
        file_count : int
            Total number of files matching the specified file extension
        &#34;&#34;&#34;
        if self.blob_storage_mounted:
            directory = join(self.mounted_blob_dir, directory)
            total_size = 0
            file_count = 0
            for filename in os.listdir(directory):
                if file_extension in filename[-len(file_extension) :]:
                    file_size = os.path.getsize(join(directory, filename)) / 1000
                    total_size += file_size
                    file_count += 1
        else:
            warnings.warn(&#34;PipelineBenchmarker does not have access to blob storage&#34;)
            total_size = 0
            file_count = 0
        return total_size, file_count

    def save_step_benchmark(
        self,
        step_name: str,
        experiment_output_dir: str,
        benchmark_dict: dict = {},
    ) -&gt; None:
        &#34;&#34;&#34;Save current step&#39;s benchmarking data.

        Parameters
        ----------
        step_name : str
            Name of the pipeline step (e.g., &#34;extract_audio&#34;)
        experiment_output_dir : str
            Directory at pipeline experiment level to store output in AML (e.g., &#34;output&#34;)
        benchmark_dict : dict
            (Optional) Dictionary containing additional metrics to save (should be flat, not nested).
            Duration, node count, and compute cluster name are automatically included so there is no need to include them in the input.
            Dictionary values should be contained within brackets (e.g., {&#34;n_files&#34;: [34]}).
        &#34;&#34;&#34;
        os.makedirs(experiment_output_dir, exist_ok=True)

        # Check if benchmark_dict is nested
        is_nested = any(isinstance(i, dict) for i in benchmark_dict.values())
        if is_nested:
            warnings.warn(
                &#34;Step benchmark values may not save correctly in the final consolidation step if benchmark_dict is nested. Please flatten the dictionary.&#34;
            )

        # Add duration and node count to dictionary
        duration_in_sec = (
            datetime.datetime.utcnow() - self.step_duration_start
        ).total_seconds()
        benchmark_dict[&#34;duration&#34;] = [format_seconds_to_timestamp(duration_in_sec)]
        n_nodes, compute_name = self.__get_n_nodes()
        benchmark_dict[&#34;n_nodes&#34;] = [n_nodes]
        benchmark_dict[&#34;compute_name&#34;] = [compute_name]
        benchmark_dict[&#34;pipeline_step&#34;] = [step_name]

        # Convert dictionary to dataframe
        df_metadata = pd.DataFrame(benchmark_dict)
        filename = f&#34;benchmark_{step_name}.csv&#34;
        metadata_filepath = join(experiment_output_dir, filename)
        df_metadata.to_csv(metadata_filepath, header=True)

        # Write to pipeline run output such that it is accessible in the final benchmarking step
        self.run.parent.upload_folder(experiment_output_dir, experiment_output_dir)
        print(
            f&#34;{filename} written to AML pipeline experiment output directory: {experiment_output_dir}&#34;
        )

    def save_pipeline_benchmark(
        self, pipeline_steps: list, experiment_output_dir: str
    ) -&gt; None:
        &#34;&#34;&#34;Consolidates all step benchmarks into one overall pipeline benchmark json.
        Uploads final json to AML output and to Azure blob storage if mounted.

        Parameters
        ----------
        pipeline_steps : list
            List of step names as defined in the pipeline (e.g., [&#34;extract_audio&#34;, &#34;process_audio&#34;])
        experiment_output_dir : str
            Directory at pipeline experiment level to store output in AML (e.g., &#34;output&#34;)
        &#34;&#34;&#34;
        os.makedirs(experiment_output_dir, exist_ok=True)

        # Initialize
        run_id = self.run.parent.id
        t_pipeline_start = datetime.datetime.strptime(
            self.pipeline_details[&#34;startTimeUtc&#34;], &#34;%Y-%m-%dT%H:%M:%S.%fZ&#34;
        )
        pipeline_duration = (
            datetime.datetime.utcnow() - t_pipeline_start
        ).total_seconds()
        compute_name, vm_size, vm_priority, region = self.__get_compute_config()

        pipeline_benchmarks = {
            &#34;datetime&#34;: datetime.datetime.now().strftime(&#34;%Y-%m-%d %H:%M:%S&#34;),
            &#34;run_id&#34;: run_id,
            &#34;pipeline_duration&#34;: format_seconds_to_timestamp(pipeline_duration),
            &#34;compute_config&#34;: {
                &#34;name&#34;: compute_name,
                &#34;vm_size&#34;: vm_size,
                &#34;vm_priority&#34;: vm_priority,
                &#34;region&#34;: region,
            },
            &#34;steps&#34;: {},
        }
        self.pipeline_benchmarks = pipeline_benchmarks

        # Add in benchmarks from each step
        max_nodes_used = 0
        for step in pipeline_steps:
            df_step_benchmark = self.__get_step_benchmark(
                experiment_output_dir,
                f&#34;benchmark_{step}.csv&#34;,
            )
            # Format into dictionary
            step_dictionary = df_step_benchmark.to_dict(orient=&#34;records&#34;)[0]
            step_dictionary.pop(&#34;Unnamed: 0&#34;)  # remove index
            pipeline_benchmarks[&#34;steps&#34;][step] = step_dictionary

            if step_dictionary[&#34;n_nodes&#34;] &gt; max_nodes_used:
                max_nodes_used = step_dictionary[&#34;n_nodes&#34;]

        pipeline_benchmarks[&#34;max_nodes_used&#34;] = max_nodes_used

        # Write to AML output
        with open(join(experiment_output_dir, &#34;benchmarks.json&#34;), &#34;w&#34;) as json_file:
            json.dump(pipeline_benchmarks, json_file)
        self.run.parent.upload_folder(experiment_output_dir, experiment_output_dir)
        print(
            f&#34;benchmarks.json written to AML pipeline experiment output directory: {experiment_output_dir}&#34;
        )

        # Write to mounted azure blob storage
        if self.blob_storage_mounted:
            folder_path = join(
                self.mounted_blob_dir, &#34;pipeline_benchmarks&#34;, &#34;individual_runs&#34;
            )
            os.makedirs(folder_path, exist_ok=True)
            filename = f&#34;benchmark_{run_id}.json&#34;

            with open(join(folder_path, filename), &#34;w&#34;) as output_file:
                json.dump(pipeline_benchmarks, output_file)
            print(f&#34;{filename} written to {folder_path}&#34;)

    def update_benchmark_table(self, table_filepath: str) -&gt; None:
        &#34;&#34;&#34;Writes or updates a table storing all the benchmarking info.

        Parameters
        ----------
        table_filepath : str
            Path to the table which contains or will contain benchmarks for all runs. It is recommended this is a filepath to somewhere in mounted blob storage.
        &#34;&#34;&#34;
        new_benchmark = self.pipeline_benchmarks

        pipeline_benchmarks = {
            &#34;datetime&#34;: [new_benchmark[&#34;datetime&#34;]],
            &#34;run_id&#34;: [new_benchmark[&#34;run_id&#34;]],
            &#34;pipeline_duration&#34;: [new_benchmark[&#34;pipeline_duration&#34;]],
            &#34;compute_name&#34;: [new_benchmark[&#34;compute_config&#34;][&#34;name&#34;]],
            &#34;compute_size&#34;: [new_benchmark[&#34;compute_config&#34;][&#34;vm_size&#34;]],
            &#34;compute_priority&#34;: [new_benchmark[&#34;compute_config&#34;][&#34;vm_priority&#34;]],
            &#34;compute_region&#34;: [new_benchmark[&#34;compute_config&#34;][&#34;region&#34;]],
        }

        df_append = pd.DataFrame()
        for step in new_benchmark[&#34;steps&#34;].keys():
            step_benchmarks = pipeline_benchmarks.copy()
            step_benchmarks[&#34;step_name&#34;] = [step]
            for step_val in new_benchmark[&#34;steps&#34;][step].keys():
                if step_val not in [
                    &#34;compute_name&#34;,
                    &#34;compute_size&#34;,
                    &#34;compute_priority&#34;,
                    &#34;compute_region&#34;,
                    &#34;pipeline_step&#34;,
                ]:
                    if step_val in [&#34;duration&#34;, &#34;n_nodes&#34;]:
                        step_benchmarks[step_val] = new_benchmark[&#34;steps&#34;][step][
                            step_val
                        ]
                    else:
                        # Preface specific (non-default) step metrics with step name
                        step_benchmarks[f&#34;{step}_{step_val}&#34;] = new_benchmark[&#34;steps&#34;][
                            step
                        ][step_val]

            df_temp = pd.DataFrame(step_benchmarks)
            df_append = df_append.append(df_temp, ignore_index=True)

        # Check if file exists
        if Path(table_filepath).is_file():
            # file exists
            df_append.to_csv(table_filepath, mode=&#34;a&#34;, header=False, index=False)
        else:
            df_append.to_csv(table_filepath, header=True, index=False)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pipelinebenchmarker.benchmark_utils.PipelineBenchmarker.get_file_size_and_count"><code class="name flex">
<span>def <span class="ident">get_file_size_and_count</span></span>(<span>self, directory: str, file_extension: str) ‑> Tuple[float, int]</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the total size of all files in a Azure blob storage directory and the number of files.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>directory</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to directory containing files (excluding path to mounted storage base directory)</dd>
<dt><strong><code>file_extension</code></strong> :&ensp;<code>str</code></dt>
<dd>Only files with the specified file extension will be counted (e.g., ".mp4")</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>total_size</code></strong> :&ensp;<code>float</code></dt>
<dd>Total size of all counted files, in kilobytes</dd>
<dt><strong><code>file_count</code></strong> :&ensp;<code>int</code></dt>
<dd>Total number of files matching the specified file extension</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_file_size_and_count(
    self, directory: str, file_extension: str
) -&gt; Tuple[float, int]:
    &#34;&#34;&#34;Gets the total size of all files in a Azure blob storage directory and the number of files.

    Parameters
    ----------
    directory : str
        Path to directory containing files (excluding path to mounted storage base directory)
    file_extension : str
        Only files with the specified file extension will be counted (e.g., &#34;.mp4&#34;)

    Returns
    -------
    total_size : float
        Total size of all counted files, in kilobytes
    file_count : int
        Total number of files matching the specified file extension
    &#34;&#34;&#34;
    if self.blob_storage_mounted:
        directory = join(self.mounted_blob_dir, directory)
        total_size = 0
        file_count = 0
        for filename in os.listdir(directory):
            if file_extension in filename[-len(file_extension) :]:
                file_size = os.path.getsize(join(directory, filename)) / 1000
                total_size += file_size
                file_count += 1
    else:
        warnings.warn(&#34;PipelineBenchmarker does not have access to blob storage&#34;)
        total_size = 0
        file_count = 0
    return total_size, file_count</code></pre>
</details>
</dd>
<dt id="pipelinebenchmarker.benchmark_utils.PipelineBenchmarker.save_pipeline_benchmark"><code class="name flex">
<span>def <span class="ident">save_pipeline_benchmark</span></span>(<span>self, pipeline_steps: list, experiment_output_dir: str) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Consolidates all step benchmarks into one overall pipeline benchmark json.
Uploads final json to AML output and to Azure blob storage if mounted.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>pipeline_steps</code></strong> :&ensp;<code>list</code></dt>
<dd>List of step names as defined in the pipeline (e.g., ["extract_audio", "process_audio"])</dd>
<dt><strong><code>experiment_output_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Directory at pipeline experiment level to store output in AML (e.g., "output")</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_pipeline_benchmark(
    self, pipeline_steps: list, experiment_output_dir: str
) -&gt; None:
    &#34;&#34;&#34;Consolidates all step benchmarks into one overall pipeline benchmark json.
    Uploads final json to AML output and to Azure blob storage if mounted.

    Parameters
    ----------
    pipeline_steps : list
        List of step names as defined in the pipeline (e.g., [&#34;extract_audio&#34;, &#34;process_audio&#34;])
    experiment_output_dir : str
        Directory at pipeline experiment level to store output in AML (e.g., &#34;output&#34;)
    &#34;&#34;&#34;
    os.makedirs(experiment_output_dir, exist_ok=True)

    # Initialize
    run_id = self.run.parent.id
    t_pipeline_start = datetime.datetime.strptime(
        self.pipeline_details[&#34;startTimeUtc&#34;], &#34;%Y-%m-%dT%H:%M:%S.%fZ&#34;
    )
    pipeline_duration = (
        datetime.datetime.utcnow() - t_pipeline_start
    ).total_seconds()
    compute_name, vm_size, vm_priority, region = self.__get_compute_config()

    pipeline_benchmarks = {
        &#34;datetime&#34;: datetime.datetime.now().strftime(&#34;%Y-%m-%d %H:%M:%S&#34;),
        &#34;run_id&#34;: run_id,
        &#34;pipeline_duration&#34;: format_seconds_to_timestamp(pipeline_duration),
        &#34;compute_config&#34;: {
            &#34;name&#34;: compute_name,
            &#34;vm_size&#34;: vm_size,
            &#34;vm_priority&#34;: vm_priority,
            &#34;region&#34;: region,
        },
        &#34;steps&#34;: {},
    }
    self.pipeline_benchmarks = pipeline_benchmarks

    # Add in benchmarks from each step
    max_nodes_used = 0
    for step in pipeline_steps:
        df_step_benchmark = self.__get_step_benchmark(
            experiment_output_dir,
            f&#34;benchmark_{step}.csv&#34;,
        )
        # Format into dictionary
        step_dictionary = df_step_benchmark.to_dict(orient=&#34;records&#34;)[0]
        step_dictionary.pop(&#34;Unnamed: 0&#34;)  # remove index
        pipeline_benchmarks[&#34;steps&#34;][step] = step_dictionary

        if step_dictionary[&#34;n_nodes&#34;] &gt; max_nodes_used:
            max_nodes_used = step_dictionary[&#34;n_nodes&#34;]

    pipeline_benchmarks[&#34;max_nodes_used&#34;] = max_nodes_used

    # Write to AML output
    with open(join(experiment_output_dir, &#34;benchmarks.json&#34;), &#34;w&#34;) as json_file:
        json.dump(pipeline_benchmarks, json_file)
    self.run.parent.upload_folder(experiment_output_dir, experiment_output_dir)
    print(
        f&#34;benchmarks.json written to AML pipeline experiment output directory: {experiment_output_dir}&#34;
    )

    # Write to mounted azure blob storage
    if self.blob_storage_mounted:
        folder_path = join(
            self.mounted_blob_dir, &#34;pipeline_benchmarks&#34;, &#34;individual_runs&#34;
        )
        os.makedirs(folder_path, exist_ok=True)
        filename = f&#34;benchmark_{run_id}.json&#34;

        with open(join(folder_path, filename), &#34;w&#34;) as output_file:
            json.dump(pipeline_benchmarks, output_file)
        print(f&#34;{filename} written to {folder_path}&#34;)</code></pre>
</details>
</dd>
<dt id="pipelinebenchmarker.benchmark_utils.PipelineBenchmarker.save_step_benchmark"><code class="name flex">
<span>def <span class="ident">save_step_benchmark</span></span>(<span>self, step_name: str, experiment_output_dir: str, benchmark_dict: dict = {}) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Save current step's benchmarking data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>step_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the pipeline step (e.g., "extract_audio")</dd>
<dt><strong><code>experiment_output_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Directory at pipeline experiment level to store output in AML (e.g., "output")</dd>
<dt><strong><code>benchmark_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>(Optional) Dictionary containing additional metrics to save (should be flat, not nested).
Duration, node count, and compute cluster name are automatically included so there is no need to include them in the input.
Dictionary values should be contained within brackets (e.g., {"n_files": [34]}).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_step_benchmark(
    self,
    step_name: str,
    experiment_output_dir: str,
    benchmark_dict: dict = {},
) -&gt; None:
    &#34;&#34;&#34;Save current step&#39;s benchmarking data.

    Parameters
    ----------
    step_name : str
        Name of the pipeline step (e.g., &#34;extract_audio&#34;)
    experiment_output_dir : str
        Directory at pipeline experiment level to store output in AML (e.g., &#34;output&#34;)
    benchmark_dict : dict
        (Optional) Dictionary containing additional metrics to save (should be flat, not nested).
        Duration, node count, and compute cluster name are automatically included so there is no need to include them in the input.
        Dictionary values should be contained within brackets (e.g., {&#34;n_files&#34;: [34]}).
    &#34;&#34;&#34;
    os.makedirs(experiment_output_dir, exist_ok=True)

    # Check if benchmark_dict is nested
    is_nested = any(isinstance(i, dict) for i in benchmark_dict.values())
    if is_nested:
        warnings.warn(
            &#34;Step benchmark values may not save correctly in the final consolidation step if benchmark_dict is nested. Please flatten the dictionary.&#34;
        )

    # Add duration and node count to dictionary
    duration_in_sec = (
        datetime.datetime.utcnow() - self.step_duration_start
    ).total_seconds()
    benchmark_dict[&#34;duration&#34;] = [format_seconds_to_timestamp(duration_in_sec)]
    n_nodes, compute_name = self.__get_n_nodes()
    benchmark_dict[&#34;n_nodes&#34;] = [n_nodes]
    benchmark_dict[&#34;compute_name&#34;] = [compute_name]
    benchmark_dict[&#34;pipeline_step&#34;] = [step_name]

    # Convert dictionary to dataframe
    df_metadata = pd.DataFrame(benchmark_dict)
    filename = f&#34;benchmark_{step_name}.csv&#34;
    metadata_filepath = join(experiment_output_dir, filename)
    df_metadata.to_csv(metadata_filepath, header=True)

    # Write to pipeline run output such that it is accessible in the final benchmarking step
    self.run.parent.upload_folder(experiment_output_dir, experiment_output_dir)
    print(
        f&#34;{filename} written to AML pipeline experiment output directory: {experiment_output_dir}&#34;
    )</code></pre>
</details>
</dd>
<dt id="pipelinebenchmarker.benchmark_utils.PipelineBenchmarker.update_benchmark_table"><code class="name flex">
<span>def <span class="ident">update_benchmark_table</span></span>(<span>self, table_filepath: str) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Writes or updates a table storing all the benchmarking info.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>table_filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the table which contains or will contain benchmarks for all runs. It is recommended this is a filepath to somewhere in mounted blob storage.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_benchmark_table(self, table_filepath: str) -&gt; None:
    &#34;&#34;&#34;Writes or updates a table storing all the benchmarking info.

    Parameters
    ----------
    table_filepath : str
        Path to the table which contains or will contain benchmarks for all runs. It is recommended this is a filepath to somewhere in mounted blob storage.
    &#34;&#34;&#34;
    new_benchmark = self.pipeline_benchmarks

    pipeline_benchmarks = {
        &#34;datetime&#34;: [new_benchmark[&#34;datetime&#34;]],
        &#34;run_id&#34;: [new_benchmark[&#34;run_id&#34;]],
        &#34;pipeline_duration&#34;: [new_benchmark[&#34;pipeline_duration&#34;]],
        &#34;compute_name&#34;: [new_benchmark[&#34;compute_config&#34;][&#34;name&#34;]],
        &#34;compute_size&#34;: [new_benchmark[&#34;compute_config&#34;][&#34;vm_size&#34;]],
        &#34;compute_priority&#34;: [new_benchmark[&#34;compute_config&#34;][&#34;vm_priority&#34;]],
        &#34;compute_region&#34;: [new_benchmark[&#34;compute_config&#34;][&#34;region&#34;]],
    }

    df_append = pd.DataFrame()
    for step in new_benchmark[&#34;steps&#34;].keys():
        step_benchmarks = pipeline_benchmarks.copy()
        step_benchmarks[&#34;step_name&#34;] = [step]
        for step_val in new_benchmark[&#34;steps&#34;][step].keys():
            if step_val not in [
                &#34;compute_name&#34;,
                &#34;compute_size&#34;,
                &#34;compute_priority&#34;,
                &#34;compute_region&#34;,
                &#34;pipeline_step&#34;,
            ]:
                if step_val in [&#34;duration&#34;, &#34;n_nodes&#34;]:
                    step_benchmarks[step_val] = new_benchmark[&#34;steps&#34;][step][
                        step_val
                    ]
                else:
                    # Preface specific (non-default) step metrics with step name
                    step_benchmarks[f&#34;{step}_{step_val}&#34;] = new_benchmark[&#34;steps&#34;][
                        step
                    ][step_val]

        df_temp = pd.DataFrame(step_benchmarks)
        df_append = df_append.append(df_temp, ignore_index=True)

    # Check if file exists
    if Path(table_filepath).is_file():
        # file exists
        df_append.to_csv(table_filepath, mode=&#34;a&#34;, header=False, index=False)
    else:
        df_append.to_csv(table_filepath, header=True, index=False)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pipelinebenchmarker" href="index.html">pipelinebenchmarker</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pipelinebenchmarker.benchmark_utils.PipelineBenchmarker" href="#pipelinebenchmarker.benchmark_utils.PipelineBenchmarker">PipelineBenchmarker</a></code></h4>
<ul class="">
<li><code><a title="pipelinebenchmarker.benchmark_utils.PipelineBenchmarker.get_file_size_and_count" href="#pipelinebenchmarker.benchmark_utils.PipelineBenchmarker.get_file_size_and_count">get_file_size_and_count</a></code></li>
<li><code><a title="pipelinebenchmarker.benchmark_utils.PipelineBenchmarker.save_pipeline_benchmark" href="#pipelinebenchmarker.benchmark_utils.PipelineBenchmarker.save_pipeline_benchmark">save_pipeline_benchmark</a></code></li>
<li><code><a title="pipelinebenchmarker.benchmark_utils.PipelineBenchmarker.save_step_benchmark" href="#pipelinebenchmarker.benchmark_utils.PipelineBenchmarker.save_step_benchmark">save_step_benchmark</a></code></li>
<li><code><a title="pipelinebenchmarker.benchmark_utils.PipelineBenchmarker.update_benchmark_table" href="#pipelinebenchmarker.benchmark_utils.PipelineBenchmarker.update_benchmark_table">update_benchmark_table</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>